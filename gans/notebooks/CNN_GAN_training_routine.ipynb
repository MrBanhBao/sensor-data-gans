{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN GAN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hao/workspace/hpi_de/4th_Semester/Applied Machine Learning/sensor-data-gans\n"
     ]
    }
   ],
   "source": [
    "cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from numpy.random import randn\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, Conv1DTranspose, MaxPool1D, GlobalMaxPool1D, Flatten, Dropout, LeakyReLU, Reshape, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from gans.gan_trainer import GanTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Discriminator and Generator Networks\n",
    "methods which creates and returns generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator(input_shape, optimizer=Adam(learning_rate=0.0001, clipvalue=1.0, decay=1e-8)):\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(Conv1D(8, kernel_size=9, activation='relu', input_shape=input_shape))\n",
    "    discriminator.add(Flatten())\n",
    "    discriminator.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator(latent_dim, kernel_size, kernel_num=3, small=True):\n",
    "    generator = Sequential()\n",
    "    generator.add(Dense(125 * 3, kernel_initializer='he_uniform', input_shape=(latent_dim,)))\n",
    "    generator.add(LeakyReLU())\n",
    "    generator.add(BatchNormalization())\n",
    "\n",
    "    generator.add(Reshape((125, 3)))\n",
    "\n",
    "    generator.add(Conv1D(kernel_num, kernel_size=kernel_size, kernel_initializer='he_uniform', padding='same'))\n",
    "    generator.add(LeakyReLU())\n",
    "    generator.add(BatchNormalization())\n",
    "\n",
    "    generator.add(Conv1DTranspose(kernel_num, kernel_size=4, strides=2, padding='same'))\n",
    "    generator.add(LeakyReLU())\n",
    "    generator.add(BatchNormalization())\n",
    "\n",
    "    generator.add(Conv1D(3, kernel_size=kernel_size, padding='same', kernel_initializer='he_uniform', activation='linear'))\n",
    "    \n",
    "    return generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameters for data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data parameters\n",
    "train_path = './datasets/mydata/train_df.h5'\n",
    "val_path = './datasets/mydata/val_df.h5'\n",
    "test_path = './datasets/mydata/test_df.h5'\n",
    "\n",
    "# what action to generate\n",
    "act_id = 0\n",
    "\n",
    "# preprocessing parameters\n",
    "window_size = 5*50\n",
    "step_size = int(window_size/2)\n",
    "method ='sliding'\n",
    "col_names = ['userAcceleration.x', 'userAcceleration.y', 'userAcceleration.z', 'userAcceleration.c']\n",
    "input_cols_train=['userAcceleration.x', 'userAcceleration.y', 'userAcceleration.z']\n",
    "input_cols_eval=['userAcceleration.c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 1000\n",
    "batch_size = 32\n",
    "eval_step = 100\n",
    "random = False\n",
    "label_smoothing = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training routines with different hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New:  ./out/cnn/act_id-0/2020-08-15 16:52:41.658018_ld-32_ks-3 Act_id: 0\n",
      "Load Data...\n",
      "Transform Data...\n",
      "Calculate origin performance...\n",
      "Done!\n",
      "discriminator loss: [0.6731553673744202, 0.4375]\n",
      "adversarial loss: 1.1896641254425049\n",
      "\n",
      "\n",
      "Train f1-score for act_id  0 improved from  0.0 to  0.7412553905126976\n",
      "Validation f1-score for act_id  0 improved from  0.0 to  0.7369323050556984\n",
      "discriminator loss: [0.48379260301589966, 0.75]\n",
      "adversarial loss: 2.479581356048584\n",
      "\n",
      "\n",
      "discriminator loss: [0.48690301179885864, 0.5625]\n",
      "adversarial loss: 1.7975653409957886\n",
      "\n",
      "\n",
      "Train f1-score for act_id  0 improved from  0.7412553905126976 to  0.749515503875969\n",
      "Validation f1-score for act_id  0 improved from  0.7369323050556984 to  0.7426597582037996\n",
      "discriminator loss: [0.5132294297218323, 0.546875]\n",
      "adversarial loss: 1.2620880603790283\n",
      "\n",
      "\n",
      "Train f1-score for act_id  0 improved from  0.749515503875969 to  0.7511531925224569\n",
      "Validation f1-score for act_id  0 improved from  0.7426597582037996 to  0.7439446366782008\n",
      "discriminator loss: [0.5558739900588989, 0.90625]\n",
      "adversarial loss: 1.005024790763855\n",
      "\n",
      "\n",
      "Train f1-score for act_id  0 improved from  0.7511531925224569 to  0.751335599805731\n",
      "discriminator loss: [0.6827055811882019, 0.5]\n",
      "adversarial loss: 0.8527640104293823\n",
      "\n",
      "\n",
      "Validation f1-score for act_id  0 improved from  0.7439446366782008 to  0.7445887445887446\n",
      "discriminator loss: [0.6928600072860718, 0.59375]\n",
      "adversarial loss: 0.7564411163330078\n",
      "\n",
      "\n",
      "discriminator loss: [0.7123810648918152, 0.328125]\n",
      "adversarial loss: 0.7008398771286011\n",
      "\n",
      "\n",
      "discriminator loss: [0.6942354440689087, 0.46875]\n",
      "adversarial loss: 0.6731366515159607\n",
      "\n",
      "\n",
      "discriminator loss: [0.6967483758926392, 0.5]\n",
      "adversarial loss: 0.6679458022117615\n",
      "\n",
      "\n",
      "New:  ./out/cnn/act_id-0/2020-08-15 16:55:35.680813_ld-32_ks-5 Act_id: 0\n",
      "Load Data...\n",
      "Transform Data...\n",
      "Calculate origin performance...\n",
      "Done!\n",
      "discriminator loss: [0.7391550540924072, 0.265625]\n",
      "adversarial loss: 0.7396693229675293\n",
      "\n",
      "\n",
      "Train f1-score for act_id  0 improved from  0.0 to  0.6981046931407943\n",
      "Validation f1-score for act_id  0 improved from  0.0 to  0.7026143790849674\n",
      "discriminator loss: [0.6208804845809937, 0.796875]\n",
      "adversarial loss: 0.9015238881111145\n",
      "\n",
      "\n",
      "Train f1-score for act_id  0 improved from  0.6981046931407943 to  0.7511531925224569\n",
      "Validation f1-score for act_id  0 improved from  0.7026143790849674 to  0.7439446366782008\n",
      "discriminator loss: [0.7054531574249268, 0.5]\n",
      "adversarial loss: 0.7122292518615723\n",
      "\n",
      "\n",
      "Train f1-score for act_id  0 improved from  0.7511531925224569 to  0.751335599805731\n",
      "Validation f1-score for act_id  0 improved from  0.7439446366782008 to  0.7445887445887446\n",
      "discriminator loss: [0.714303731918335, 0.515625]\n",
      "adversarial loss: 0.6775716543197632\n",
      "\n",
      "\n",
      "discriminator loss: [0.6956949234008789, 0.4375]\n",
      "adversarial loss: 0.6684339642524719\n",
      "\n",
      "\n",
      "discriminator loss: [0.7187013626098633, 0.15625]\n",
      "adversarial loss: 0.6675671339035034\n",
      "\n",
      "\n",
      "discriminator loss: [0.6985360383987427, 0.453125]\n",
      "adversarial loss: 0.668572723865509\n",
      "\n",
      "\n",
      "discriminator loss: [0.6956490278244019, 0.484375]\n",
      "adversarial loss: 0.6698089838027954\n",
      "\n",
      "\n",
      "discriminator loss: [0.6939536929130554, 0.5]\n",
      "adversarial loss: 0.6711450815200806\n",
      "\n",
      "\n",
      "discriminator loss: [0.6939244270324707, 0.5]\n",
      "adversarial loss: 0.6725751757621765\n",
      "\n",
      "\n",
      "New:  ./out/cnn/act_id-0/2020-08-15 16:58:52.870586_ld-32_ks-7 Act_id: 0\n",
      "Load Data...\n",
      "Transform Data...\n",
      "Calculate origin performance...\n",
      "Done!\n",
      "discriminator loss: [0.582338809967041, 0.5]\n",
      "adversarial loss: 1.616292953491211\n",
      "\n",
      "\n",
      "Train f1-score for act_id  0 improved from  0.0 to  0.7880794701986754\n",
      "Validation f1-score for act_id  0 improved from  0.0 to  0.7644444444444444\n",
      "discriminator loss: [0.6036803722381592, 0.671875]\n",
      "adversarial loss: 0.8801069855690002\n",
      "\n",
      "\n",
      "discriminator loss: [0.7261017560958862, 0.09375]\n",
      "adversarial loss: 0.6781784892082214\n",
      "\n",
      "\n",
      "discriminator loss: [0.7014990448951721, 0.375]\n",
      "adversarial loss: 0.670210599899292\n",
      "\n",
      "\n",
      "discriminator loss: [0.6955118179321289, 0.46875]\n",
      "adversarial loss: 0.6706806421279907\n",
      "\n",
      "\n",
      "discriminator loss: [0.7055424451828003, 0.375]\n",
      "adversarial loss: 0.6716339588165283\n",
      "\n",
      "\n",
      "discriminator loss: [0.6981757283210754, 0.453125]\n",
      "adversarial loss: 0.6726520657539368\n",
      "\n",
      "\n",
      "discriminator loss: [0.6949858665466309, 0.484375]\n",
      "adversarial loss: 0.6736664175987244\n",
      "\n",
      "\n",
      "discriminator loss: [0.6936348676681519, 0.5]\n",
      "adversarial loss: 0.6748155355453491\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3): # repeats\n",
    "    for act_id in range(0,1): # for each action\n",
    "        for latent_dim in [32, 64, 128, 256]: # different latent dimensions\n",
    "            for kernel_size in [3, 5, 7, 9, 11, 13, 15]: # different kernel sizes\n",
    "                \n",
    "                out_dir = './out/cnn/act_id-{}/{}_ld-{}_ks-{}'.format(act_id, datetime.now(), latent_dim, kernel_size)\n",
    "                print(\"New: \", out_dir, 'Act_id:', act_id)\n",
    "                \n",
    "                gan_trainer = GanTrainer(out_dir=out_dir)\n",
    "                gan_trainer.init_data(train_path=train_path,\n",
    "                      val_path=val_path,\n",
    "                      test_path=test_path,\n",
    "                      act_id=act_id,\n",
    "                      window_size=window_size,\n",
    "                      step_size=step_size,\n",
    "                      method=method,\n",
    "                      col_names=col_names,\n",
    "                      input_cols_train=input_cols_train,\n",
    "                      input_cols_eval=input_cols_eval\n",
    "                     )\n",
    "                \n",
    "                input_shape = gan_trainer.x_train[0].shape\n",
    "\n",
    "                discriminator = create_discriminator(input_shape)\n",
    "                generator = create_generator(latent_dim, kernel_size=kernel_size)\n",
    "                gan_trainer.create_gan(generator=generator, discriminator=discriminator)\n",
    "\n",
    "                gan_trainer.train_gan(steps=steps, \n",
    "                          batch_size=batch_size, \n",
    "                          eval_step=eval_step,\n",
    "                          random=random, \n",
    "                          label_smoothing=label_smoothing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
